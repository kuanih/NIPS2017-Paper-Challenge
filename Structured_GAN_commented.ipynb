{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIPS Paper Implementation Challenge \n",
    "## PyTorch Code Implementation for Paper Structured Generative Adversarial Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO [12:46:29] __main__: PyTorch version: 0.2.0_3\n"
     ]
    }
   ],
   "source": [
    "### IMPORTS ###\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "import utils\n",
    "# initialize logger\n",
    "import logging.config\n",
    "import yaml\n",
    "with open('./log_config.yaml') as file:\n",
    "    Dict = yaml.load(file)    # load config file\n",
    "    logging.config.dictConfig(Dict)    # import config\n",
    "    \n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info('PyTorch version: ' + str(torch.__version__))\n",
    "\n",
    "# import SGAN utils\n",
    "from layers import rampup, rampdown\n",
    "from zca import ZCA\n",
    "from models import Generator, InferenceNet, ClassifierNet, DConvNet1, DConvNet2\n",
    "from trainGAN import pretrain_classifier, train_classifier, train_gan, eval_classifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.autograd import Variable\n",
    "import logging\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import weight_norm as wn\n",
    "\n",
    "from layers import conv_concat, mlp_concat, init_weights, Gaussian_NoiseLayer, MeanOnlyBatchNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Parameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO [12:46:29] __main__: Cuda = False\n"
     ]
    }
   ],
   "source": [
    "### GLOBAL PARAMS ###\n",
    "BATCH_SIZE = 200\n",
    "BATCH_SIZE_EVAL = 200\n",
    "NUM_CLASSES = 10\n",
    "NUM_LABELLED = 4000\n",
    "SSL_SEED = 1\n",
    "NP_SEED = 1234\n",
    "CUDA = torch.cuda.is_available()\n",
    "logger.info('Cuda = ' + str(CUDA))\n",
    "\n",
    "# data dependent\n",
    "IN_CHANNELS = 3\n",
    "\n",
    "# evaluation\n",
    "VIS_EPOCH = 1\n",
    "EVAL_EPOCH = 1\n",
    "\n",
    "# C\n",
    "SCALED_UNSUP_WEIGHT_MAX = 100.0\n",
    "\n",
    "# G\n",
    "N_Z = 100\n",
    "\n",
    "# optimization\n",
    "B1 = 0.5  # beta1 in Adam\n",
    "LR = 3e-4\n",
    "LR_CLA = 3e-3\n",
    "NUM_EPOCHS = 1000\n",
    "NUM_EPOCHS_PRE = 20\n",
    "ANNEAL_EPOCH = 200\n",
    "ANNEAL_EVERY_EPOCH = 1\n",
    "ANNEAL_FACTOR = 0.995\n",
    "ANNEAL_FACTOR_CLA = 0.99\n",
    "\n",
    "path_out = \"./results\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "- Download the cifar-10 dataset from 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "- Split the dataset into labelled training and test dataset, with length of 50000 and 10000 resprectively\n",
    "- Create an unlabelled dataset by copying the training dataset created above\n",
    "- Shuffle the labelled training dataset\n",
    "- Create a much smaller length of labelled training dataset with length of 4000 for our semi-supervised classification setting.\n",
    "- Calculate the number of minibatches for labelled training and test, and unlabelled training datasets respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO [12:46:31] __main__: Loading data...\n"
     ]
    }
   ],
   "source": [
    "### DATA ###\n",
    "logger.info('Loading data...')\n",
    "train_x, train_y = utils.load('./cifar10/', 'train')\n",
    "eval_x, eval_y = utils.load('./cifar10/', 'test')\n",
    "\n",
    "train_y = np.int32(train_y)\n",
    "eval_y = np.int32(eval_y)\n",
    "x_unlabelled = train_x.copy()\n",
    "\n",
    "rng_data = np.random.RandomState(SSL_SEED)\n",
    "inds = rng_data.permutation(train_x.shape[0])\n",
    "train_x = train_x[inds]\n",
    "train_y = train_y[inds]\n",
    "x_labelled = []\n",
    "y_labelled = []\n",
    "\n",
    "for j in range(NUM_CLASSES):\n",
    "    x_labelled.append(train_x[train_y == j][:int(NUM_LABELLED / NUM_CLASSES)])\n",
    "    y_labelled.append(train_y[train_y == j][:int(NUM_LABELLED / NUM_CLASSES)])\n",
    "\n",
    "x_labelled = np.concatenate(x_labelled, axis=0)\n",
    "y_labelled = np.concatenate(y_labelled, axis=0)\n",
    "del train_x\n",
    "\n",
    "num_batches_l = int(x_labelled.shape[0] // BATCH_SIZE)\n",
    "num_batches_u = int(x_unlabelled.shape[0] // BATCH_SIZE)\n",
    "num_batches_e = int(eval_x.shape[0] // BATCH_SIZE_EVAL)\n",
    "rng = np.random.RandomState(NP_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Structures\n",
    "The model structures of structured GAN consist of one generator network, two discriminator networks, and two inference networks which are named as classifier and inferentor respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator Network $p_g (x \\mid y, z)$\n",
    "The generator parametrizes the sampling process $x \\sim p_g(x \\mid y, z) = G(y, z)$ by taking some hidden structures $y$ of $x$ and other factors $z$ as inputs, and outputing generated samples $x$. \n",
    "![title](generator_paper.png)\n",
    "\n",
    "The generator architecture consists of following layers, activation functions, and parameters.\n",
    "![title](generator.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### MODEL STRUCTURES ###\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, dense_neurons, weight_init=True):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.logger = logging.getLogger(__name__)  # initialize logger\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.Dense = nn.Linear(input_size, dense_neurons)\n",
    "        self.Relu = nn.ReLU()\n",
    "        self.Tanh = nn.Tanh()\n",
    "\n",
    "        self.Deconv2D_0 = nn.ConvTranspose2d(in_channels=522, out_channels=256,\n",
    "                                             kernel_size=5, stride=2, padding=2,\n",
    "                                             output_padding=1, bias=False)\n",
    "        self.Deconv2D_1 = nn.ConvTranspose2d(in_channels=266, out_channels=128,\n",
    "                                             kernel_size=5, stride=2, padding=2,\n",
    "                                             output_padding=1, bias=False)\n",
    "        self.Deconv2D_2 = wn(nn.ConvTranspose2d(in_channels=138, out_channels=3,\n",
    "                                                kernel_size=5, stride=2, padding=2,\n",
    "                                                output_padding=1, bias=False))\n",
    "\n",
    "        self.BatchNorm1D = nn.BatchNorm1d(dense_neurons)\n",
    "\n",
    "        self.BatchNorm2D_0 = nn.BatchNorm2d(256)\n",
    "        self.BatchNorm2D_1 = nn.BatchNorm2d(128)\n",
    "\n",
    "        if weight_init:\n",
    "            # initialize weights for all conv and lin layers\n",
    "            self.apply(init_weights)\n",
    "            # log network structure\n",
    "            self.logger.debug(self)\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        x = mlp_concat(z, y, self.num_classes)\n",
    "\n",
    "        x = self.Dense(x)\n",
    "        x = self.Relu(x)\n",
    "        x = self.BatchNorm1D(x)\n",
    "\n",
    "        x = x.resize(z.size(0), 512, 4, 4)\n",
    "        x = conv_concat(x, y, self.num_classes)\n",
    "\n",
    "        x = self.Deconv2D_0(x)                    # output shape (256,8,8) = 8192 * 2\n",
    "        x = self.Relu(x)\n",
    "        x = self.BatchNorm2D_0(x)\n",
    "\n",
    "        x = conv_concat(x, y, self.num_classes)\n",
    "\n",
    "        x = self.Deconv2D_1(x)                    # output shape (128,16,16) = 8192 * 2 * 2\n",
    "        x = self.Relu(x)\n",
    "        x = self.BatchNorm2D_1(x)\n",
    "\n",
    "        x = conv_concat(x, y, self.num_classes)\n",
    "        x = self.Deconv2D_2(x)                    # output shape (3, 32, 32) = 3072\n",
    "        x = self.Tanh(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator Network 1\n",
    "\n",
    "Discriminator 1 is trained to distinguish generated pairs $(x, y) \\sim p_g(x,y)$ by using generator introduced above from those come from real pairs $p(x,y)$. It takes $x$ and $y$ as inputs and outputs the classification decisions whether the joint pairs are drawn from $p_g(x,y)$ or $p(x,y)$.\n",
    "\n",
    "![title](discriminator1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# discriminator xy2p: test a pair of input comes from p(x, y) instead of p_c or p_g\n",
    "class DConvNet1(nn.Module):\n",
    "    '''\n",
    "    1st convolutional discriminative net (discriminator xy2p)\n",
    "    --> does a pair of input come from p(x, y) instead of p_c or p_g ?\n",
    "    '''\n",
    "\n",
    "    def __init__(self, channel_in, num_classes, p_dropout=0.2, weight_init=True):\n",
    "        super(DConvNet1, self).__init__()\n",
    "\n",
    "        self.logger = logging.getLogger(__name__)  # initialize logger\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # general reusable layers:\n",
    "        self.LReLU = nn.LeakyReLU(negative_slope=0.2)  # leaky ReLU activation function\n",
    "        self.sgmd = nn.Sigmoid()  # sigmoid activation function\n",
    "        self.drop = nn.Dropout(p=p_dropout)  # dropout layer\n",
    "\n",
    "        # input -->\n",
    "        # drop\n",
    "        # ConvConcat\n",
    "\n",
    "        self.conv1 = wn(nn.Conv2d(in_channels=channel_in + num_classes, out_channels=32,\n",
    "                                  kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False))\n",
    "        # LReLU\n",
    "        # ConvConcat\n",
    "\n",
    "        self.conv2 = wn(nn.Conv2d(in_channels=32 + num_classes, out_channels=32,\n",
    "                                  kernel_size=(3, 3), stride=2, padding=1, bias=False))\n",
    "        # LReLU\n",
    "        # drop\n",
    "        # ConvConcat\n",
    "\n",
    "        self.conv3 = wn(nn.Conv2d(in_channels=32 + num_classes, out_channels=64,\n",
    "                                  kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False))\n",
    "        # LReLU\n",
    "        # ConvConcat\n",
    "\n",
    "        self.conv4 = wn(nn.Conv2d(in_channels=64 + num_classes, out_channels=64,\n",
    "                                  kernel_size=(3, 3), stride=2, padding=1, bias=False))\n",
    "        # LReLU\n",
    "        # drop\n",
    "        # ConvConcat\n",
    "\n",
    "        self.conv5 = wn(nn.Conv2d(in_channels=64 + num_classes, out_channels=128,\n",
    "                                  kernel_size=(3, 3), stride=(1, 1), padding=0, bias=False))\n",
    "        # LReLU\n",
    "        # ConvConcat\n",
    "\n",
    "        self.conv6 = wn(nn.Conv2d(in_channels=128 + num_classes, out_channels=128,\n",
    "                                  kernel_size=(3, 3), stride=(1, 1), padding=0, bias=False))\n",
    "        # LReLU\n",
    "\n",
    "        self.globalPool = nn.AdaptiveAvgPool2d(output_size=4)\n",
    "\n",
    "        # MLPConcat\n",
    "\n",
    "        self.lin = nn.Linear(in_features=128 * 4 * 4 + num_classes,\n",
    "                             out_features=1)\n",
    "        # smg\n",
    "\n",
    "        if weight_init:\n",
    "            # initialize weights for all conv and lin layers\n",
    "            self.apply(init_weights)\n",
    "            # log network structure\n",
    "            self.logger.debug(self)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # x: (bs, channel_in, dim_input)\n",
    "        # y: (bs, 1)\n",
    "\n",
    "        x0 = self.drop(x)\n",
    "        x0 = conv_concat(x0, y, self.num_classes)\n",
    "\n",
    "        x1 = self.LReLU(self.conv1(x0))\n",
    "        x1 = conv_concat(x1, y, self.num_classes)\n",
    "\n",
    "        x2 = self.LReLU(self.conv2(x1))\n",
    "        x2 = self.drop(x2)\n",
    "        x2 = conv_concat(x2, y, self.num_classes)\n",
    "\n",
    "        x3 = self.LReLU(self.conv3(x2))\n",
    "        x3 = conv_concat(x3, y, self.num_classes)\n",
    "\n",
    "        x4 = self.LReLU(self.conv4(x3))\n",
    "        x4 = self.drop(x4)\n",
    "        x4 = conv_concat(x4, y, self.num_classes)\n",
    "\n",
    "        x5 = self.LReLU(self.conv5(x4))\n",
    "        x5 = conv_concat(x5, y, self.num_classes)\n",
    "\n",
    "        x6 = self.LReLU(self.conv6(x5))\n",
    "\n",
    "        x_pool = self.globalPool(x6)\n",
    "\n",
    "        x_pool = x_pool.view(-1, 128 * 4 * 4)\n",
    "        x_out = mlp_concat(x_pool, y, self.num_classes)\n",
    "\n",
    "        out = self.sgmd(self.lin(x_out))\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator Network 2\n",
    "\n",
    "Discriminator 2 is trained to distinguish generated pairs $(x, z) \\sim p_g(x,z)$ by using generator introduced above from those come from inferenced pairs $p_i (x,z)$ by using inferentor introduced below. It takes $x$ and $z$ as inputs and outputs the classification decisions whether the joint pairs are drawn from $p_g(x,z)$ or $p_i(x,z)$.\n",
    "![title](discriminator2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# discriminator xz\n",
    "class DConvNet2(nn.Module):\n",
    "    '''\n",
    "    2nd convolutional discriminative net (discriminator xz)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_z, channel_in, num_classes, weight_init=True):\n",
    "        super(DConvNet2, self).__init__()\n",
    "\n",
    "        self.logger = logging.getLogger(__name__)  # initialize logger\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # general reusable layers:\n",
    "        self.LReLU = nn.LeakyReLU(negative_slope=0.2)  # leaky ReLU activation function\n",
    "        self.sgmd = nn.Sigmoid()  # sigmoid activation function\n",
    "\n",
    "        # z input -->\n",
    "        self.lin_z0 = nn.Linear(in_features=n_z,\n",
    "                                out_features=512)\n",
    "        # LReLU\n",
    "\n",
    "        self.lin_z1 = nn.Linear(in_features=512,\n",
    "                                out_features=512)\n",
    "        # LReLU\n",
    "\n",
    "        # -------------------------------------\n",
    "\n",
    "        # x input -->\n",
    "        self.conv_x0 = nn.Conv2d(in_channels=channel_in, out_channels=128,\n",
    "                                 kernel_size=(5, 5), stride=2, padding=2, bias=False)\n",
    "        # LReLU\n",
    "\n",
    "        self.conv_x1 = nn.Conv2d(in_channels=128, out_channels=256,\n",
    "                                 kernel_size=(5, 5), stride=2, padding=2, bias=False)\n",
    "        # LReLU\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=256)\n",
    "\n",
    "        self.conv_x2 = nn.Conv2d(in_channels=256, out_channels=512,\n",
    "                                 kernel_size=(5, 5), stride=2, padding=2, bias=False)\n",
    "        # LReLU\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=512)\n",
    "\n",
    "        # -------------------------------------\n",
    "\n",
    "        # concat x & z -->\n",
    "        self.lin_f0 = nn.Linear(in_features=8704,\n",
    "                                out_features=1024)\n",
    "        # LReLU\n",
    "\n",
    "        self.lin_f1 = nn.Linear(in_features=1024,\n",
    "                                out_features=1)\n",
    "        # smg\n",
    "\n",
    "        if weight_init:\n",
    "            # initialize weights for all conv and lin layers\n",
    "            self.apply(init_weights)\n",
    "            # log network structure\n",
    "            self.logger.debug(self)\n",
    "\n",
    "    def forward(self, z, x):\n",
    "        # x: (bs, channel_in, dim_input)\n",
    "        # z: (bs, n_z)\n",
    "\n",
    "        z0 = self.LReLU(self.lin_z0(z))\n",
    "        z_out = self.LReLU(self.lin_z1(z0))\n",
    "\n",
    "        x0 = self.LReLU(self.conv_x0(x))\n",
    "        x1 = self.LReLU(self.conv_x1(x0))\n",
    "        x1 = self.bn1(x1)\n",
    "        x_out = self.LReLU(self.conv_x2(x1))\n",
    "        x_out = self.bn2(x_out)\n",
    "\n",
    "        dims = x_out.size()\n",
    "        fusion = torch.cat([x_out.view(dims[0], -1).squeeze(-1).squeeze(-1), z_out], dim=1)\n",
    "\n",
    "        f_out = self.LReLU(self.lin_f0(fusion))\n",
    "        out = self.sgmd(self.lin_f1(f_out))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference Networks\n",
    "Two inference networks define two distributions $p_i(z\\mid x)$ and $p_c(y\\mid x)$ that are used to approximate the true posterior $p(z\\mid x)$ and $p(y \\mid x)$ using two different adversarial games, which are named InferenceNet and ClassifierNet accordingly. \n",
    "![title](inferencenets.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class InferenceNet(nn.Module):\n",
    "    def __init__(self, in_channels, n_z, weight_init=True):\n",
    "        super(InferenceNet, self).__init__()\n",
    "\n",
    "        self.logger = logging.getLogger(__name__)  # initialize logger\n",
    "\n",
    "        self.inf02 = nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=4,\n",
    "                               stride=2, padding=1)\n",
    "        self.inf03 = nn.BatchNorm2d(64)\n",
    "        self.inf11 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4,\n",
    "                               stride=2, padding=1)\n",
    "        self.inf12 = nn.BatchNorm2d(128)\n",
    "        self.inf21 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4,\n",
    "                               stride=2, padding=1)\n",
    "        self.inf22 = nn.BatchNorm2d(256)\n",
    "        self.inf31 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4,\n",
    "                               stride=2, padding=1)\n",
    "        self.inf32 = nn.BatchNorm2d(512)\n",
    "        self.inf4 = nn.Linear(in_features=512*2*2, out_features=n_z)\n",
    "\n",
    "        if weight_init:\n",
    "            # initialize weights for all conv and lin layers\n",
    "            self.apply(init_weights)\n",
    "            # log network structure\n",
    "            self.logger.debug(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.inf03(self.inf02(x)))\n",
    "        x = F.leaky_relu(self.inf12(self.inf11(x)))\n",
    "        x = F.leaky_relu(self.inf22(self.inf21(x)))\n",
    "        x = F.leaky_relu(self.inf32(self.inf31(x)))\n",
    "        x = x.view(-1, 512*2*2)\n",
    "        x = self.inf4(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier\n",
    "The classifier is a inference network $C: x \\rightarrow y$ which approximates the posterior $p(y\\mid x)$ as $y \\sim p_c(y \\mid x) =C(x)$. In the case of using cifar-10 dataset, it is a 10-way classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classifier module\n",
    "class ClassifierNet(nn.Module):\n",
    "    def __init__(self, in_channels, weight_init=True):\n",
    "        super(ClassifierNet, self).__init__()\n",
    "\n",
    "        self.logger = logging.getLogger(__name__)  # initialize logger\n",
    "\n",
    "        self.gaussian = Gaussian_NoiseLayer()\n",
    "\n",
    "        self.conv1a = nn.Conv2d(in_channels=in_channels, out_channels=128, kernel_size=3,\n",
    "                                stride=1, padding=1)\n",
    "        self.convWN1 = MeanOnlyBatchNorm([1, 128, 32, 32])\n",
    "        self.conv1b = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,\n",
    "                                stride=1, padding=1)\n",
    "        self.conv_relu = nn.LeakyReLU(negative_slope=0.1)\n",
    "        self.conv1c = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,\n",
    "                                stride=1, padding=1)\n",
    "        self.conv_maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.dropout1 = nn.Dropout2d(p=0.5)\n",
    "        self.conv2a = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3,\n",
    "                                stride=1, padding=1)\n",
    "        self.convWN2 = MeanOnlyBatchNorm([1, 256, 16, 16])\n",
    "        self.conv2b = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,\n",
    "                                stride=1, padding=1)\n",
    "        self.conv2c = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,\n",
    "                                stride=1, padding=1)\n",
    "        self.conv_maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.dropout2 = nn.Dropout2d(p=0.5)\n",
    "        self.conv3a = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3,\n",
    "                                stride=1, padding=0)  # output[6,6]\n",
    "        self.convWN3a = MeanOnlyBatchNorm([1, 512, 6, 6])\n",
    "        self.conv3b = nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1,\n",
    "                                stride=1, padding=0)\n",
    "        self.convWN3b = MeanOnlyBatchNorm([1, 256, 6, 6])\n",
    "        self.conv3c = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1,\n",
    "                                stride=1, padding=0)\n",
    "        self.convWN3c = MeanOnlyBatchNorm([1, 128, 6, 6])\n",
    "\n",
    "        self.conv_globalpool = nn.AdaptiveAvgPool2d(6)\n",
    "\n",
    "        self.dense = nn.Linear(in_features=128 * 6 * 6, out_features=10)\n",
    "        self.smx = nn.Softmax()\n",
    "        #self.WNfinal = MeanOnlyBatchNorm([1, 128, 6, 6])\n",
    "\n",
    "        if weight_init:\n",
    "            # initialize weights for all conv and lin layers\n",
    "            self.apply(init_weights)\n",
    "            # log network structure\n",
    "            self.logger.debug(self)\n",
    "\n",
    "    def forward(self, x, cuda):\n",
    "        x = self.gaussian(x, cuda=cuda)\n",
    "        x = self.convWN1(self.conv_relu(self.conv1a(x)))\n",
    "        x = self.convWN1(self.conv_relu(self.conv1b(x)))\n",
    "        x = self.convWN1(self.conv_relu(self.conv1c(x)))\n",
    "        x = self.conv_maxpool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.convWN2(self.conv_relu(self.conv2a(x)))\n",
    "        x = self.convWN2(self.conv_relu(self.conv2b(x)))\n",
    "        x = self.convWN2(self.conv_relu(self.conv2c(x)))\n",
    "        x = self.conv_maxpool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.convWN3a(self.conv_relu(self.conv3a(x)))\n",
    "        x = self.convWN3b(self.conv_relu(self.conv3b(x)))\n",
    "        x = self.convWN3c(self.conv_relu(self.conv3c(x)))\n",
    "        x = self.conv_globalpool(x)\n",
    "        x = x.view(-1, 128 * 6 * 6)\n",
    "        #x = self.WNfinal(self.smx(self.dense(x)))\n",
    "        x = self.smx(self.dense(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Process\n",
    "The training process of structured GAN consists of pretraining of classifier, and training classifier, discriminator, inferentor, and generator in succession. It envolves training two adversarial games $\\mathcal{L}_{xy}$ and $\\mathcal{L}_{xz}$, and two collorative games $\\mathcal{R}_y$ and $\\mathcal{R}_z$. In following subsections, training details for different networks, such loss functions and optimizers used, are described. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_gan(discriminator1, discriminator2, generator, inferentor, classifier, whitener,\n",
    "              x_labelled, x_unlabelled, y_labelled, p_u_d, p_u_i,\n",
    "              num_classes, batch_size, num_batches_u,\n",
    "              batch_c, batch_l, batch_g,\n",
    "              n_z, optimizers, losses, rng, cuda=False):\n",
    "\n",
    "    '''\n",
    "\n",
    "    Args:\n",
    "        discriminator1(DConvNet1): Discriminator instance xy\n",
    "        discriminator2(DConvNet2): Discriminator instance xz\n",
    "        generator(Generator): Generator instance\n",
    "        inferentor(InferenceNet): Inference Net instance\n",
    "        classifier(ClassifierNet): Classifier Net instance\n",
    "        whitener(ZCA): ZCA instance\n",
    "        x_labelled: batch of labelled input data\n",
    "        x_unlabelled: batch of unlabelled input data\n",
    "        y_labelled: batch of corresponding labels\n",
    "        p_u_d: data slice object (idx)\n",
    "        p_u_i: data slice object (idx)\n",
    "        num_classes(int): number of target classes\n",
    "        batch_size(int): size of mini-batch\n",
    "        num_batches_u:\n",
    "        batch_c:\n",
    "        batch_l:\n",
    "        batch_g:\n",
    "        n_z:\n",
    "        optimizers(dict): dictionary containing optimizer instances for all respective nets (dis, gen, inf)\n",
    "        losses(dict): dictionary containing respective loss instances (BCE, MSE, CE)\n",
    "        b1: beta1 in Adam\n",
    "        cuda(bool): cuda flag\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    '''\n",
    "\n",
    "    for i in range(num_batches_u):\n",
    "            i_l = i % (x_labelled.shape[0] // batch_l)\n",
    "\n",
    "            from_u_i = i*batch_size  # unlabelled inferentor slice\n",
    "            to_u_i = (i+1)*batch_size\n",
    "            from_u_d = i*batch_c    # unlabelled discriminator slice\n",
    "            to_u_d = (i+1) * batch_c\n",
    "            from_l = i_l*batch_l    # labelled\n",
    "            to_l = (i_l+1)*batch_l\n",
    "\n",
    "            # create samples and labels\n",
    "            sample_y = torch.from_numpy(np.int32(np.repeat(np.arange(num_classes), int(batch_size/num_classes))))\n",
    "            y_real = torch.from_numpy(np.int32(np.random.randint(10, size=batch_g)))\n",
    "            z_real = torch.from_numpy(np.random.uniform(size=(batch_g, n_z)).astype(np.float32))\n",
    "            z_rand = torch.rand((batch_size*n_z)).view(batch_size, n_z)\n",
    "\n",
    "            sample_y, y_real, z_real, z_rand = Variable(sample_y), Variable(y_real), Variable(z_real), Variable(z_rand)\n",
    "            if cuda:\n",
    "                sample_y, y_real, z_real, z_rand = sample_y.cuda(), y_real.cuda(), z_real.cuda(), z_rand.cuda()\n",
    "\n",
    "            dis_losses = train_discriminator(discriminator1=discriminator1,\n",
    "                                             discriminator2=discriminator2,\n",
    "                                             generator=generator,\n",
    "                                             inferentor=inferentor,\n",
    "                                             classificator=classifier,\n",
    "                                             whitener=whitener,\n",
    "                                             x_labelled=x_labelled[from_l:to_l],  # sym_x_l\n",
    "                                             x_unlabelled=x_unlabelled,\n",
    "                                             y_labelled=y_labelled[from_l:to_l],  # sym_y\n",
    "                                             slice_x_dis=p_u_d[from_u_d:to_u_d],  # slice_x_u_d\n",
    "                                             y_real=y_real,  # sym_y_m\n",
    "                                             z_real=z_real,  # sym_z_m\n",
    "                                             slice_x_inf=p_u_i[from_u_i:to_u_i],  # slice_x_u_i\n",
    "                                             sample_y=sample_y,  # sym_y_g\n",
    "                                             z_rand=z_rand,\n",
    "                                             batch_size=batch_size,\n",
    "                                             optimizer=optimizers['dis'],\n",
    "                                             loss=losses['bce'],\n",
    "                                             cuda=cuda)\n",
    "\n",
    "            inf_losses = train_inferentor(x_unlabelled=x_unlabelled,\n",
    "                                          sample_y=sample_y,\n",
    "                                          generator=generator,\n",
    "                                          z_rand=z_rand,\n",
    "                                          discriminator2=discriminator2,\n",
    "                                          inferentor=inferentor,\n",
    "                                          mse=losses['mse'],\n",
    "                                          bce=losses['bce'],\n",
    "                                          slice_x_u_i=p_u_i[from_u_i:to_u_i],\n",
    "                                          optimizer=optimizers['inf'],\n",
    "                                          cuda=cuda)\n",
    "\n",
    "            gen_losses = train_generator(whitener=whitener,\n",
    "                                         optimizer=optimizers['gen'],\n",
    "                                         BCE_loss=losses['bce'],\n",
    "                                         MSE_loss=losses['mse'],\n",
    "                                         cross_entropy_loss=losses['ce'],\n",
    "                                         discriminator1=discriminator1,\n",
    "                                         discriminator2=discriminator2,\n",
    "                                         inferentor=inferentor,\n",
    "                                         generator=generator,\n",
    "                                         classifier=classifier,\n",
    "                                         sample_y=sample_y,\n",
    "                                         z_rand=z_rand,\n",
    "                                         cuda=cuda)\n",
    "\n",
    "\n",
    "            gan_loss = {\n",
    "                'dis': dis_losses,\n",
    "                'inf': inf_losses,\n",
    "                'gen': gen_losses\n",
    "            }\n",
    "\n",
    "            return gan_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretraining\n",
    "Pretraining aims to obtain a relative good prior weights for later training phase. It is achieved by minimizing the reconstruction error of $y$ in terms of $C$, on both labeled data $X_l$ and generated data:  $$\\min_{C,G} \\mathcal{R}_y = - \\mathbb{E}_{(x,y)\\sim p(x,y)}[\\log p_c(y\\mid x)]$$\n",
    "\n",
    "Adam optimizer is used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pretrain_classifier(x_labelled, x_unlabelled, y_labelled, eval_x, eval_y, num_batches_l,\n",
    "                        batch_size, num_batches_u, classifier, whitener, losses, rng, cuda):\n",
    "\n",
    "    # randomly permute data and labels\n",
    "    permutation_labelled = rng.permutation(x_labelled.shape[0])\n",
    "    x_labelled = x_labelled[permutation_labelled]\n",
    "    y_labelled = y_labelled[permutation_labelled]\n",
    "    permutation_unlabelled = rng.permutation(x_unlabelled.shape[0]).astype('int32')\n",
    "\n",
    "    x_labelled = Variable(torch.from_numpy(x_labelled))\n",
    "    y_labelled = Variable(torch.from_numpy(y_labelled))\n",
    "\n",
    "    eval_x = Variable(torch.from_numpy(eval_x))\n",
    "    eval_y = Variable(torch.from_numpy(eval_y))\n",
    "    x_unlabelled = Variable(torch.from_numpy(x_unlabelled))\n",
    "\n",
    "    if cuda:\n",
    "        x_labelled, y_labelled, eval_x, eval_y, x_unlabelled = \\\n",
    "            x_labelled.cuda(), y_labelled.cuda(), eval_x.cuda(), eval_y.cuda(), x_unlabelled.cuda()\n",
    "\n",
    "    for i in range(num_batches_u):\n",
    "        i_c = i % num_batches_l\n",
    "        x_l = x_labelled[i_c * batch_size:(i_c + 1) * batch_size]\n",
    "        x_l_zca = whitener.apply(x_l)\n",
    "        y = y_labelled[i_c * batch_size:(i_c + 1) * batch_size]\n",
    "        y = y.type(torch.LongTensor)\n",
    "        if cuda:\n",
    "            y = y.cuda()\n",
    "\n",
    "        # classify input\n",
    "        cla_out_y_l = classifier(x_l_zca, cuda=cuda)\n",
    "        # calculate loss\n",
    "        cla_cost_l = losses['ce'](cla_out_y_l, y)\n",
    "\n",
    "        batch_slicer = torch.from_numpy(permutation_unlabelled[i * batch_size:(i + 1) * batch_size]).type(torch.LongTensor)\n",
    "        if cuda:\n",
    "            batch_slicer = batch_slicer.cuda()\n",
    "        x_u_rep = x_unlabelled[batch_slicer]\n",
    "        x_u_rep_zca = whitener.apply(x_u_rep)\n",
    "        # classify input\n",
    "        cla_out_y_rep = classifier(x_u_rep_zca, cuda=cuda)\n",
    "        target = cla_out_y_rep.detach()\n",
    "        del cla_out_y_rep\n",
    "\n",
    "        x_u = x_unlabelled[batch_slicer]\n",
    "        x_u_zca = whitener.apply(x_u)\n",
    "        # classify input\n",
    "        cla_out_y = classifier(x_u_zca, cuda=cuda)\n",
    "\n",
    "        # calculate loss\n",
    "        cla_cost_u = 100 * losses['mse'](cla_out_y, target)\n",
    "\n",
    "        # sum losses\n",
    "        pretrain_cost = cla_cost_l + cla_cost_u\n",
    "\n",
    "        # run optimization and update weights\n",
    "        cla_optimizer = optim.Adam(classifier.parameters(), betas=(0.9, 0.999), lr=3e-3)  # they implement robust adam\n",
    "        pretrain_cost.backward()\n",
    "        cla_optimizer.step()\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_generator(whitener, optimizer, BCE_loss, MSE_loss, cross_entropy_loss,\n",
    "                    discriminator1, discriminator2, inferentor, generator, classifier, sample_y, z_rand, cuda):\n",
    "    '''\n",
    "    Args:\n",
    "        whitener(ZCA):      ZCA instance\n",
    "        optimizer:          optimizer  for generator\n",
    "        BCE_loss:           binary cross entropy loss\n",
    "        MSE_loss:           mean squared error loss\n",
    "        cross_entropy_loss: cross entropy loss\n",
    "        discriminator1(DConvNet1): Discriminator instance xy\n",
    "        discriminator2(DConvNet2): Discriminator instance xz\n",
    "        inferentor:          Inference net\n",
    "        generator:          Generator net\n",
    "        classifier:         Classificaiton net\n",
    "        sample_y:           sampled labels\n",
    "        z_rand:             random z sample\n",
    "\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    '''\n",
    "    # compute loss\n",
    "    gen_out_x = generator(z_rand, sample_y)\n",
    "    inf_z_g = inferentor(gen_out_x)\n",
    "    gen_out_x_zca = whitener.apply(gen_out_x)\n",
    "    cla_out_y_g = classifier(gen_out_x_zca, cuda=cuda)\n",
    "    rz = MSE_loss(inf_z_g, z_rand)\n",
    "    sample_y = sample_y.long()\n",
    "    ry = cross_entropy_loss(cla_out_y_g, sample_y)\n",
    "    dis_out_p_g = discriminator1(x=gen_out_x, y=sample_y)\n",
    "    disxz_out_p_g = discriminator2(z=z_rand, x=gen_out_x)\n",
    "\n",
    "    target1, target2 = Variable(torch.ones(dis_out_p_g.size())), Variable(torch.ones(disxz_out_p_g.size()))\n",
    "    if cuda:\n",
    "        target1, target2 = target1.cuda(), target2.cuda()\n",
    "\n",
    "    gen_cost_p_g_1 = BCE_loss(dis_out_p_g, target1)\n",
    "    gen_cost_p_g_2 = BCE_loss(disxz_out_p_g, target2)\n",
    "\n",
    "    generator_cost = gen_cost_p_g_1 + gen_cost_p_g_2 + rz + ry\n",
    "\n",
    "    # optimization routines and weight updates\n",
    "    optimizer.zero_grad()\n",
    "    generator_cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return generator_cost.cpu().data.numpy().mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_discriminator(discriminator1, discriminator2, generator, inferentor, classificator, whitener,\n",
    "                        x_labelled, x_unlabelled, y_labelled,\n",
    "                        slice_x_dis, y_real, z_real, slice_x_inf, sample_y, z_rand,\n",
    "                        batch_size, optimizer, loss, cuda):\n",
    "    '''\n",
    "\n",
    "    Args:\n",
    "        discriminator1(DConvNet1): Discriminator instance xy\n",
    "        discriminator2(DConvNet2): Discriminator instance xz\n",
    "        generator(Generator): Generator instance\n",
    "        inferentor(InferenceNet): Inference Net instance\n",
    "        classificator(ClassifierNet): Classifier Net instance\n",
    "        whitener(ZCA): ZCA instance\n",
    "        x_labelled: batch of labelled input data\n",
    "        x_unlabelled: batch of unlabelled input data\n",
    "        y_labelled: batch of corresponding labels\n",
    "        slice_x_dis: indexes to select unlabelled data for discriminator\n",
    "        y_real: class labels\n",
    "        z_real: generator_x_m noise input\n",
    "        slice_x_inf: indexes to select unlabelled data for inference net\n",
    "        sample_y: sampled labels\n",
    "        z_rand: generator_x noise input\n",
    "        batch_size(int): size of mini-batch\n",
    "        d1_optimizer(torch.optim): optimizer instance for discriminator1\n",
    "        d2_optimizer(torch.optim): optimizer instance for discriminator2\n",
    "        loss(torch.nn.Loss): loss instance for discriminators (BCE)\n",
    "        cuda(bool): cuda flag (GPU)\n",
    "\n",
    "    Returns: list(discriminator1 loss, discriminator2 loss)\n",
    "\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Parameter Translation: Theano original --> PyTorch\n",
    "    input:\n",
    "        x_labelled[from_l:to_l],  # sym_x_l\n",
    "        y_labelled[from_l:to_l],  # sym_y\n",
    "        p_u_d[from_u_d:to_u_d] --> slice_x_dis,  # slice_x_u_d\n",
    "        y_real,  # sym_y_m\n",
    "        z_real,  # sym_z_m\n",
    "        p_u_i[from_u_i:to_u_i] --> slice_x_inf,  # slice_x_u_i\n",
    "        sample_y,  # sym_y_g\n",
    "    '''\n",
    "\n",
    "\n",
    "    # get respective data slices for batch\n",
    "    unlabel_dis = x_unlabelled[slice_x_dis]  # original: sym_x_u_d\n",
    "    unlabel_dis_zca = whitener.apply(unlabel_dis)  # original: sym_x_u_d_zca\n",
    "    unlabel_inf = x_unlabelled[slice_x_inf]  # original: sym_x_u_i\n",
    "\n",
    "    # convert data ndarrays to pytorch tensor variables\n",
    "    x_labelled = Variable(torch.from_numpy(x_labelled))\n",
    "    y_labelled = Variable(torch.from_numpy(y_labelled))\n",
    "    unlabel_dis = Variable(torch.from_numpy(unlabel_dis))\n",
    "    unlabel_dis_zca = Variable(torch.from_numpy(unlabel_dis_zca))\n",
    "    unlabel_inf = Variable(torch.from_numpy(unlabel_inf))\n",
    "\n",
    "    if cuda:\n",
    "        x_labelled, y_labelled = x_labelled.cuda(), y_labelled.cuda()\n",
    "        unlabel_dis, unlabel_dis_zca, unlabel_inf = unlabel_dis.cuda(), unlabel_dis_zca.cuda(), unlabel_inf.cuda()\n",
    "\n",
    "    # generate samples\n",
    "    gen_out_x = generator(z=z_rand, y=sample_y)\n",
    "    gen_out_x_m = generator(z=z_real, y=y_real)\n",
    "\n",
    "    # compute inference\n",
    "    inf_z = inferentor(unlabel_inf)\n",
    "\n",
    "    # classify\n",
    "    cla_out = classificator(unlabel_dis_zca, cuda=cuda)\n",
    "    cla_out_val, cla_out_idx = cla_out.max(dim=1)\n",
    "\n",
    "    # concatenate inputs\n",
    "    x_in = torch.cat([x_labelled, unlabel_dis, gen_out_x_m], dim=0)[:batch_size]\n",
    "\n",
    "    y_labelled = y_labelled.long()\n",
    "    y_real = y_real.long()\n",
    "    y_in = torch.cat([y_labelled, cla_out_idx, y_real], dim=0)[:batch_size]\n",
    "\n",
    "    # calculate probabilities by discriminators\n",
    "    dis1_out_p = discriminator1(x=x_in, y=y_in)\n",
    "    dis1_out_pg = discriminator1(x=gen_out_x, y=sample_y)\n",
    "\n",
    "    dis2_out_p = discriminator2(z=inf_z, x=unlabel_inf)\n",
    "    dis2_out_pg = discriminator2(z=z_rand, x=gen_out_x)\n",
    "\n",
    "    # create discriminator labels\n",
    "    p_label_d1 = Variable(torch.ones(dis1_out_p.size()))\n",
    "    pg_label_d1 = Variable(torch.zeros(dis1_out_pg.size()))\n",
    "    p_label_d2 = Variable(torch.ones(dis2_out_p.size()))\n",
    "    pg_label_d2 = Variable(torch.zeros(dis2_out_pg.size()))\n",
    "\n",
    "    if cuda:\n",
    "        p_label_d1, pg_label_d1, \\\n",
    "        p_label_d2, pg_label_d2 = p_label_d1.cuda(), pg_label_d1.cuda(), \\\n",
    "                                  p_label_d2.cuda(), pg_label_d2.cuda()\n",
    "\n",
    "    # compute loss\n",
    "    dis1_cost_p = loss(dis1_out_p, p_label_d1)\n",
    "    dis1_cost_pg = loss(dis1_out_pg, pg_label_d1)\n",
    "    dis2_cost_p = loss(dis2_out_p, p_label_d2)\n",
    "    dis2_cost_pg = loss(dis2_out_pg, pg_label_d2)\n",
    "\n",
    "    # sum individual losses\n",
    "    dis1_cost = dis1_cost_p + dis1_cost_pg  # for report\n",
    "    dis2_cost = dis2_cost_p + dis2_cost_pg  # for report\n",
    "    total_cost = dis1_cost + dis2_cost\n",
    "\n",
    "    # optimization routines and weight updates\n",
    "    optimizer.zero_grad()\n",
    "    total_cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return total_cost.cpu().data.numpy().mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_classifier(x_labelled, y_labelled, x_unlabelled, num_batches_u, eval_epoch,\n",
    "                     size_l, size_u, size_g, n_z, whitener, classifier, p_u,\n",
    "                     unsup_weight, losses, generator, w_g, cla_lr, rng, b1_c, cuda):\n",
    "    '''\n",
    "\n",
    "    Args:\n",
    "        x_labelled: batch of labelled input data\n",
    "        y_labelled: batch of labels\n",
    "        x_unlabelled: unlabelled data\n",
    "        num_batches_u:\n",
    "        eval_epoch:\n",
    "        size_l:\n",
    "        size_u:\n",
    "        size_g:\n",
    "        n_z:\n",
    "        whitener:\n",
    "        classifier: Classifier Net instance\n",
    "        p_u:\n",
    "        unsup_weight:\n",
    "        losses(dict): dictionary containing respective loss instances (BCE, MSE, CE)\n",
    "        generator:\n",
    "        w_g:\n",
    "        cla_lr:\n",
    "        rng:\n",
    "        b1_c:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    '''\n",
    "\n",
    "    running_cla_cost = 0.0\n",
    "    epochs = num_batches_u * eval_epoch\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "        i_l = i % (x_labelled.shape[0] // size_l)\n",
    "        i_u = i % (x_unlabelled.shape[0] // size_u)\n",
    "\n",
    "        y_real = np.int32(np.random.randint(10, size=size_g))\n",
    "        z_real = np.random.uniform(size=(size_g, n_z)).astype(np.float32)\n",
    "\n",
    "        x_l = x_labelled[i_l * size_l:(i_l + 1) * size_l]\n",
    "        y = y_labelled[i_l * size_l:(i_l + 1) * size_l]\n",
    "        x_l_zca = whitener.apply(x_l)\n",
    "\n",
    "        slice_x_u_c = p_u[i_u*size_u:(i_u+1)*size_u]\n",
    "        x_u_rep = x_unlabelled[slice_x_u_c]  # copy x_u_zca? double assigned variable??\n",
    "        x_u = x_unlabelled[slice_x_u_c]\n",
    "        x_u_rep_zca = whitener.apply(x_u_rep)\n",
    "        x_u_zca = whitener.apply(x_u)\n",
    "\n",
    "        # convert to torch tensor variable\n",
    "        y_real = Variable(torch.from_numpy(y_real))\n",
    "        z_real = Variable(torch.from_numpy(z_real))\n",
    "        y = Variable(torch.from_numpy(y).type(torch.LongTensor))\n",
    "        x_l_zca = Variable(torch.from_numpy(x_l_zca))\n",
    "        x_u_rep_zca = Variable(torch.from_numpy(x_u_rep_zca))\n",
    "        x_u_zca = Variable(torch.from_numpy(x_u_zca))\n",
    "        if cuda:\n",
    "            y_real, z_real, y = y_real.cuda(), z_real.cuda(), y.cuda()\n",
    "            x_l_zca, x_u_rep_zca, x_u_zca = x_l_zca.cuda(), x_u_rep_zca.cuda(), x_u_zca.cuda()\n",
    "\n",
    "        # classify input\n",
    "        cla_out_y_l = classifier(x_l_zca, cuda=cuda)\n",
    "        # calculate loss\n",
    "        cla_cost_l = losses['ce'](cla_out_y_l, y)  # size_average in pytorch is by default\n",
    "\n",
    "        # classify input for target\n",
    "        cla_out_y_rep = classifier(x_u_rep_zca, cuda=cuda)\n",
    "        target = cla_out_y_rep.detach()\n",
    "        del cla_out_y_rep\n",
    "\n",
    "        # classify input\n",
    "        cla_out_y = classifier(x_u_zca, cuda=cuda)\n",
    "        # calculate loss\n",
    "        cla_cost_u = unsup_weight * losses['mse'](cla_out_y, target)\n",
    "\n",
    "        y_m = y_real.type(torch.LongTensor)\n",
    "        z_m = z_real\n",
    "        if cuda:\n",
    "            y_m, z_m = y_m.cuda(), z_m.cuda()\n",
    "        gen_out_x_m = generator(z=z_m, y=y_m)\n",
    "        gen_out_x_m_zca = whitener.apply(gen_out_x_m)\n",
    "\n",
    "        # classify input\n",
    "        cla_out_y_m = classifier(gen_out_x_m_zca, cuda=cuda)\n",
    "        # calculate loss\n",
    "        cla_cost_g = losses['ce'](cla_out_y_m, y_m) * float(w_g)\n",
    "\n",
    "        # sum individual losses for backward\n",
    "        cla_cost = cla_cost_l + cla_cost_u + cla_cost_g\n",
    "\n",
    "        cla_optimizer = optim.Adam(classifier.parameters(), betas=(b1_c, 0.999), lr=cla_lr)\n",
    "        # zero the parameter gradients, optimize and update parameters\n",
    "        cla_optimizer.zero_grad()\n",
    "        cla_cost.backward()\n",
    "        cla_optimizer.step()\n",
    "\n",
    "        # update batch permutations\n",
    "        if i_l == ((x_labelled.shape[0] // size_l) - 1):\n",
    "            p_l = rng.permutation(x_labelled.shape[0])\n",
    "            x_labelled = x_labelled[p_l]\n",
    "            y_labelled = y_labelled[p_l]\n",
    "        if i_u == (num_batches_u - 1):\n",
    "            p_u = rng.permutation(x_unlabelled.shape[0]).astype('int32')\n",
    "\n",
    "        running_cla_cost += cla_cost.cpu().data.numpy().mean()\n",
    "\n",
    "    return running_cla_cost/epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Inferentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_inferentor(x_unlabelled, sample_y, generator, z_rand, discriminator2, inferentor,\n",
    "                     mse, bce, slice_x_u_i, optimizer, cuda):\n",
    "\n",
    "    x_u_i = x_unlabelled[slice_x_u_i]\n",
    "    x_u_i = Variable(torch.from_numpy(x_u_i))\n",
    "\n",
    "    if cuda:\n",
    "        x_u_i = x_u_i.cuda()\n",
    "\n",
    "    y_g = sample_y\n",
    "    gen_out_x = generator(z_rand, y_g)\n",
    "    inf_z = inferentor(x_u_i)\n",
    "    inf_z_g = inferentor(gen_out_x)\n",
    "    disxz_out_p = discriminator2(z=inf_z, x=x_u_i)\n",
    "    target = inf_z_g.detach()\n",
    "    rz = mse(z_rand, target)\n",
    "\n",
    "    target = Variable(torch.zeros(disxz_out_p.size()))\n",
    "    if cuda:\n",
    "        target = target.cuda()\n",
    "    inf_cost_p_i = bce(disxz_out_p, target)\n",
    "    inf_cost = inf_cost_p_i + rz\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    inf_cost.backward()\n",
    "    optimizer.step()\n",
    "    return inf_cost.cpu().data.numpy().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_classifier(num_batches_e, eval_x, eval_y, batch_size, whitener, classifier, cuda):\n",
    "\n",
    "    accurracy = []\n",
    "    for i in range(num_batches_e):\n",
    "        x_eval = eval_x[i*batch_size:(i+1)*batch_size]\n",
    "        y_eval = eval_y[i*batch_size:(i+1)*batch_size]\n",
    "        x_eval_zca = whitener.apply(x_eval)\n",
    "        x_eval_zca = Variable(torch.from_numpy(x_eval_zca))\n",
    "        if cuda:\n",
    "            x_eval_zca = x_eval_zca.cuda()\n",
    "\n",
    "        cla_out_y_eval = classifier(x_eval_zca, cuda=cuda)\n",
    "\n",
    "        pred = cla_out_y_eval.cpu().data.numpy()\n",
    "        pred = np.argmax(pred, axis=1)\n",
    "\n",
    "        accurracy_batch = accuracy_score(y_eval, pred)\n",
    "        accurracy.append(accurracy_batch)\n",
    "\n",
    "    return np.mean(accurracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### INITS ###\n",
    "\n",
    "# GENRATOR\n",
    "generator = Generator(input_size=110, num_classes=NUM_CLASSES, dense_neurons=(4 * 4 * 512))\n",
    "\n",
    "# INFERENCE\n",
    "inference = InferenceNet(in_channels=IN_CHANNELS, n_z=N_Z)\n",
    "\n",
    "# CLASSIFIER\n",
    "classifier = ClassifierNet(in_channels=IN_CHANNELS)\n",
    "\n",
    "# DISCRIMINATOR\n",
    "discriminator1 = DConvNet1(channel_in=IN_CHANNELS, num_classes=NUM_CLASSES)\n",
    "discriminator2 = DConvNet2(n_z=N_Z, channel_in=IN_CHANNELS, num_classes=NUM_CLASSES)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put on GPU\n",
    "if CUDA:\n",
    "    generator.cuda()\n",
    "    inference.cuda()\n",
    "    classifier.cuda()\n",
    "    discriminator1.cuda()\n",
    "    discriminator2.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ZCA\n",
    "whitener = ZCA(x=x_unlabelled)\n",
    "\n",
    "# LOSS FUNCTIONS\n",
    "if CUDA:\n",
    "    losses = {\n",
    "        'bce': nn.BCELoss().cuda(),\n",
    "        'mse': nn.MSELoss().cuda(),\n",
    "        'ce': nn.CrossEntropyLoss().cuda()\n",
    "    }\n",
    "else:\n",
    "    losses = {\n",
    "        'bce': nn.BCELoss(),\n",
    "        'mse': nn.MSELoss(),\n",
    "        'ce': nn.CrossEntropyLoss()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### PRETRAIN CLASSIFIER ###\n",
    "\n",
    "logger.info('Start pretraining...')\n",
    "for epoch in range(1, 1+NUM_EPOCHS_PRE):\n",
    "\n",
    "    # pretrain classifier net\n",
    "    classifier = pretrain_classifier(x_labelled, x_unlabelled, y_labelled, eval_x, eval_y, num_batches_l,\n",
    "                                     BATCH_SIZE, num_batches_u, classifier, whitener, losses, rng, CUDA)\n",
    "\n",
    "    # evaluate\n",
    "    accurracy = eval_classifier(num_batches_e, eval_x, eval_y, BATCH_SIZE_EVAL, whitener, classifier, CUDA)\n",
    "\n",
    "    logger.info(str(epoch) + ':Pretrain error_rate: ' + str(1 - accurracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "### GAN TRAINING ###\n",
    "\n",
    "# assign start values\n",
    "lr_cla = LR_CLA\n",
    "lr = LR\n",
    "start_full = time.time()\n",
    "\n",
    "logger.info(\"Start GAN training...\")\n",
    "for epoch in range(1, 1+NUM_EPOCHS):\n",
    "\n",
    "    # OPTIMIZERS\n",
    "    optimizers = {\n",
    "        'dis': optim.Adam(list(discriminator1.parameters()) + list(discriminator2.parameters()), betas=(B1, 0.999), lr=lr),\n",
    "        'gen': optim.Adam(generator.parameters(), betas=(B1, 0.999), lr=lr),\n",
    "        'inf': optim.Adam(inference.parameters(), betas=(B1, 0.999), lr=lr)\n",
    "    }\n",
    "\n",
    "    # randomly permute data and labels each epoch\n",
    "    p_l = rng.permutation(x_labelled.shape[0])\n",
    "    x_labelled = x_labelled[p_l]\n",
    "    y_labelled = y_labelled[p_l]\n",
    "\n",
    "    # permuted slicer objects\n",
    "    p_u = rng.permutation(x_unlabelled.shape[0]).astype('int32')\n",
    "    p_u_d = rng.permutation(x_unlabelled.shape[0]).astype('int32')\n",
    "    p_u_i = rng.permutation(x_unlabelled.shape[0]).astype('int32')\n",
    "\n",
    "    # set epoch dependent values\n",
    "    if epoch < (NUM_EPOCHS/2):\n",
    "        if epoch % 50 == 1:\n",
    "            batch_l = 200 - (epoch // 50 + 1) * 16\n",
    "            batch_c = (epoch // 50 + 1) * 16\n",
    "            batch_g = 1\n",
    "    elif epoch < NUM_EPOCHS and epoch % 100 == 0:\n",
    "        batch_l = 50\n",
    "        batch_c = 140 - 10 * (epoch-500)/100\n",
    "        batch_g = 10 + 10 * (epoch-500)/100\n",
    "\n",
    "    # if current epoch is an evaluation epoch, train classifier and report results\n",
    "    if epoch % EVAL_EPOCH == 0:\n",
    "\n",
    "        logger.info('Train classifier...')\n",
    "\n",
    "        rampup_value = rampup(epoch-1)\n",
    "        rampdown_value = rampdown(epoch-1)\n",
    "        b1_c = rampdown_value * 0.9 + (1.0 - rampdown_value) * 0.5\n",
    "        unsup_weight = rampup_value * SCALED_UNSUP_WEIGHT_MAX if epoch > 1 else 0.0\n",
    "        w_g = np.float32(min(float(epoch) / 300.0, 1.0))\n",
    "\n",
    "        size_l = 100\n",
    "        size_g = 100\n",
    "        size_u = 100\n",
    "\n",
    "        cla_losses = train_classifier(x_labelled=x_labelled,\n",
    "                                      y_labelled=y_labelled,\n",
    "                                      x_unlabelled=x_unlabelled,\n",
    "                                      num_batches_u=num_batches_u,\n",
    "                                      eval_epoch=EVAL_EPOCH,\n",
    "                                      size_l=size_l,\n",
    "                                      size_u=size_u,\n",
    "                                      size_g=size_g,\n",
    "                                      n_z=N_Z,\n",
    "                                      whitener=whitener,\n",
    "                                      classifier=classifier,\n",
    "                                      p_u=p_u,\n",
    "                                      unsup_weight=unsup_weight,\n",
    "                                      losses=losses,\n",
    "                                      generator=generator,\n",
    "                                      w_g=w_g,\n",
    "                                      cla_lr=lr_cla,\n",
    "                                      rng=rng,\n",
    "                                      b1_c=b1_c,\n",
    "                                      cuda=CUDA)\n",
    "\n",
    "        # evaluate & report\n",
    "        accurracy = eval_classifier(num_batches_e, eval_x, eval_y, BATCH_SIZE_EVAL, whitener, classifier, CUDA)\n",
    "\n",
    "        logger.info('Evaluation error_rate: %.5f\\n' % (1 - accurracy))\n",
    "\n",
    "    logger.info('Train generator, inference and discriminator model...')\n",
    "    # train GAN model\n",
    "    for i in range(num_batches_u):\n",
    "        gan_losses = train_gan(discriminator1=discriminator1,\n",
    "                               discriminator2=discriminator2,\n",
    "                               generator=generator,\n",
    "                               inferentor=inference,\n",
    "                               classifier=classifier,\n",
    "                               whitener=whitener,\n",
    "                               x_labelled=x_labelled,\n",
    "                               x_unlabelled=x_unlabelled,\n",
    "                               y_labelled=y_labelled,\n",
    "                               p_u_d=p_u_d,\n",
    "                               p_u_i=p_u_i,\n",
    "                               num_classes=NUM_CLASSES,\n",
    "                               batch_size=BATCH_SIZE,\n",
    "                               num_batches_u=num_batches_u,\n",
    "                               batch_c=batch_c,\n",
    "                               batch_l=batch_l,\n",
    "                               batch_g=batch_g,\n",
    "                               n_z=N_Z,\n",
    "                               optimizers=optimizers,\n",
    "                               losses=losses,\n",
    "                               rng=rng,\n",
    "                               cuda=CUDA)\n",
    "\n",
    "    # anneal the learning rates\n",
    "    if (epoch >= ANNEAL_EPOCH) and (epoch % ANNEAL_EVERY_EPOCH == 0):\n",
    "        lr = lr * ANNEAL_FACTOR\n",
    "        lr_cla *= ANNEAL_FACTOR_CLA\n",
    "\n",
    "    # report and log training info\n",
    "    t = time.time() - start_full\n",
    "    line = \"*Epoch=%d Time=%.2f LR=%.5f\\n\" % (epoch, t, lr) + \"DisLosses: \" + str(gan_losses['dis']) + \"\\nGenLosses: \" + \\\n",
    "           str(gan_losses['gen']) + \"\\nInfLosses: \" + str(gan_losses['inf']) + \"\\nClaLosses: \" + str(cla_losses)\n",
    "    logger.info(line)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
