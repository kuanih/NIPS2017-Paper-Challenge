{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIPS Paper Implementation Challenge \n",
    "## PyTorch Code Implementation for Paper Structured Generative Adversarial Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO [16:56:14] __main__: PyTorch version: 0.2.0_3\n"
     ]
    }
   ],
   "source": [
    "### IMPORTS ###\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "import utils\n",
    "# initialize logger\n",
    "import logging.config\n",
    "import yaml\n",
    "with open('./log_config.yaml') as file:\n",
    "    Dict = yaml.load(file)    # load config file\n",
    "    logging.config.dictConfig(Dict)    # import config\n",
    "    \n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info('PyTorch version: ' + str(torch.__version__))\n",
    "\n",
    "# import SGAN utils\n",
    "from layers import rampup, rampdown\n",
    "from zca import ZCA\n",
    "from models import Generator, InferenceNet, ClassifierNet, DConvNet1, DConvNet2\n",
    "from trainGAN import pretrain_classifier, train_classifier, train_gan, eval_classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Parameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO [17:05:11] __main__: Cuda = False\n"
     ]
    }
   ],
   "source": [
    "### GLOBAL PARAMS ###\n",
    "BATCH_SIZE = 200\n",
    "BATCH_SIZE_EVAL = 200\n",
    "NUM_CLASSES = 10\n",
    "NUM_LABELLED = 4000\n",
    "SSL_SEED = 1\n",
    "NP_SEED = 1234\n",
    "CUDA = torch.cuda.is_available()\n",
    "logger.info('Cuda = ' + str(CUDA))\n",
    "\n",
    "# data dependent\n",
    "IN_CHANNELS = 3\n",
    "\n",
    "# evaluation\n",
    "VIS_EPOCH = 1\n",
    "EVAL_EPOCH = 1\n",
    "\n",
    "# C\n",
    "SCALED_UNSUP_WEIGHT_MAX = 100.0\n",
    "\n",
    "# G\n",
    "N_Z = 100\n",
    "\n",
    "# optimization\n",
    "B1 = 0.5  # moment1 in Adam\n",
    "LR = 3e-4\n",
    "LR_CLA = 3e-3\n",
    "NUM_EPOCHS = 1000\n",
    "NUM_EPOCHS_PRE = 20\n",
    "ANNEAL_EPOCH = 200\n",
    "ANNEAL_EVERY_EPOCH = 1\n",
    "ANNEAL_FACTOR = 0.995\n",
    "ANNEAL_FACTOR_CLA = 0.99\n",
    "\n",
    "path_out = \"./results\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO [16:56:47] __main__: Cuda = False\n",
      "   INFO [16:56:47] __main__: Loading data...\n",
      ">> Downloading cifar-10-python.tar.gz 100.0%\n",
      "Successfully downloaded cifar-10-python.tar.gz 170498071 bytes.\n"
     ]
    }
   ],
   "source": [
    "### DATA ###\n",
    "logger.info('Loading data...')\n",
    "train_x, train_y = utils.load('./cifar10/', 'train')\n",
    "eval_x, eval_y = utils.load('./cifar10/', 'test')\n",
    "\n",
    "train_y = np.int32(train_y)\n",
    "eval_y = np.int32(eval_y)\n",
    "x_unlabelled = train_x.copy()\n",
    "\n",
    "rng_data = np.random.RandomState(SSL_SEED)\n",
    "inds = rng_data.permutation(train_x.shape[0])\n",
    "train_x = train_x[inds]\n",
    "train_y = train_y[inds]\n",
    "x_labelled = []\n",
    "y_labelled = []\n",
    "\n",
    "for j in range(NUM_CLASSES):\n",
    "    x_labelled.append(train_x[train_y == j][:int(NUM_LABELLED / NUM_CLASSES)])\n",
    "    y_labelled.append(train_y[train_y == j][:int(NUM_LABELLED / NUM_CLASSES)])\n",
    "\n",
    "x_labelled = np.concatenate(x_labelled, axis=0)\n",
    "y_labelled = np.concatenate(y_labelled, axis=0)\n",
    "del train_x\n",
    "\n",
    "num_batches_l = int(x_labelled.shape[0] // BATCH_SIZE)\n",
    "num_batches_u = int(x_unlabelled.shape[0] // BATCH_SIZE)\n",
    "num_batches_e = int(eval_x.shape[0] // BATCH_SIZE_EVAL)\n",
    "rng = np.random.RandomState(NP_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Structures\n",
    "#### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### MODEL STRUCTURES ###\n",
    "\n",
    "# generator y2x: p_g(x, y) = p(y) p_g(x | y) where x = G(z, y), z follows p_g(z)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, dense_neurons, weight_init=True):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.logger = logging.getLogger(__name__)  # initialize logger\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.Dense = nn.Linear(input_size, dense_neurons)\n",
    "        self.Relu = nn.ReLU()\n",
    "        self.Tanh = nn.Tanh()\n",
    "\n",
    "        self.Deconv2D_0 = nn.ConvTranspose2d(in_channels=522, out_channels=256,\n",
    "                                             kernel_size=5, stride=2, padding=2,\n",
    "                                             output_padding=1, bias=False)\n",
    "        self.Deconv2D_1 = nn.ConvTranspose2d(in_channels=266, out_channels=128,\n",
    "                                             kernel_size=5, stride=2, padding=2,\n",
    "                                             output_padding=1, bias=False)\n",
    "        self.Deconv2D_2 = wn(nn.ConvTranspose2d(in_channels=138, out_channels=3,\n",
    "                                                kernel_size=5, stride=2, padding=2,\n",
    "                                                output_padding=1, bias=False))\n",
    "\n",
    "        self.BatchNorm1D = nn.BatchNorm1d(dense_neurons)\n",
    "\n",
    "        self.BatchNorm2D_0 = nn.BatchNorm2d(256)\n",
    "        self.BatchNorm2D_1 = nn.BatchNorm2d(128)\n",
    "\n",
    "        if weight_init:\n",
    "            # initialize weights for all conv and lin layers\n",
    "            self.apply(init_weights)\n",
    "            # log network structure\n",
    "            self.logger.debug(self)\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        x = mlp_concat(z, y, self.num_classes)\n",
    "\n",
    "        x = self.Dense(x)\n",
    "        x = self.Relu(x)\n",
    "        x = self.BatchNorm1D(x)\n",
    "\n",
    "        x = x.resize(z.size(0), 512, 4, 4)\n",
    "        x = conv_concat(x, y, self.num_classes)\n",
    "\n",
    "        x = self.Deconv2D_0(x)                    # output shape (256,8,8) = 8192 * 2\n",
    "        x = self.Relu(x)\n",
    "        x = self.BatchNorm2D_0(x)\n",
    "\n",
    "        x = conv_concat(x, y, self.num_classes)\n",
    "\n",
    "        x = self.Deconv2D_1(x)                    # output shape (128,16,16) = 8192 * 2 * 2\n",
    "        x = self.Relu(x)\n",
    "        x = self.BatchNorm2D_1(x)\n",
    "\n",
    "        x = conv_concat(x, y, self.num_classes)\n",
    "        x = self.Deconv2D_2(x)                    # output shape (3, 32, 32) = 3072\n",
    "        x = self.Tanh(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# discriminator xy2p: test a pair of input comes from p(x, y) instead of p_c or p_g\n",
    "class DConvNet1(nn.Module):\n",
    "    '''\n",
    "    1st convolutional discriminative net (discriminator xy2p)\n",
    "    --> does a pair of input come from p(x, y) instead of p_c or p_g ?\n",
    "    '''\n",
    "\n",
    "    def __init__(self, channel_in, num_classes, p_dropout=0.2, weight_init=True):\n",
    "        super(DConvNet1, self).__init__()\n",
    "\n",
    "        self.logger = logging.getLogger(__name__)  # initialize logger\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # general reusable layers:\n",
    "        self.LReLU = nn.LeakyReLU(negative_slope=0.2)  # leaky ReLU activation function\n",
    "        self.sgmd = nn.Sigmoid()  # sigmoid activation function\n",
    "        self.drop = nn.Dropout(p=p_dropout)  # dropout layer\n",
    "\n",
    "        # input -->\n",
    "        # drop\n",
    "        # ConvConcat\n",
    "\n",
    "        self.conv1 = wn(nn.Conv2d(in_channels=channel_in + num_classes, out_channels=32,\n",
    "                                  kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False))\n",
    "        # LReLU\n",
    "        # ConvConcat\n",
    "\n",
    "        self.conv2 = wn(nn.Conv2d(in_channels=32 + num_classes, out_channels=32,\n",
    "                                  kernel_size=(3, 3), stride=2, padding=1, bias=False))\n",
    "        # LReLU\n",
    "        # drop\n",
    "        # ConvConcat\n",
    "\n",
    "        self.conv3 = wn(nn.Conv2d(in_channels=32 + num_classes, out_channels=64,\n",
    "                                  kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False))\n",
    "        # LReLU\n",
    "        # ConvConcat\n",
    "\n",
    "        self.conv4 = wn(nn.Conv2d(in_channels=64 + num_classes, out_channels=64,\n",
    "                                  kernel_size=(3, 3), stride=2, padding=1, bias=False))\n",
    "        # LReLU\n",
    "        # drop\n",
    "        # ConvConcat\n",
    "\n",
    "        self.conv5 = wn(nn.Conv2d(in_channels=64 + num_classes, out_channels=128,\n",
    "                                  kernel_size=(3, 3), stride=(1, 1), padding=0, bias=False))\n",
    "        # LReLU\n",
    "        # ConvConcat\n",
    "\n",
    "        self.conv6 = wn(nn.Conv2d(in_channels=128 + num_classes, out_channels=128,\n",
    "                                  kernel_size=(3, 3), stride=(1, 1), padding=0, bias=False))\n",
    "        # LReLU\n",
    "\n",
    "        self.globalPool = nn.AdaptiveAvgPool2d(output_size=4)\n",
    "\n",
    "        # MLPConcat\n",
    "\n",
    "        self.lin = nn.Linear(in_features=128 * 4 * 4 + num_classes,\n",
    "                             out_features=1)\n",
    "        # smg\n",
    "\n",
    "        if weight_init:\n",
    "            # initialize weights for all conv and lin layers\n",
    "            self.apply(init_weights)\n",
    "            # log network structure\n",
    "            self.logger.debug(self)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # x: (bs, channel_in, dim_input)\n",
    "        # y: (bs, 1)\n",
    "\n",
    "        x0 = self.drop(x)\n",
    "        x0 = conv_concat(x0, y, self.num_classes)\n",
    "\n",
    "        x1 = self.LReLU(self.conv1(x0))\n",
    "        x1 = conv_concat(x1, y, self.num_classes)\n",
    "\n",
    "        x2 = self.LReLU(self.conv2(x1))\n",
    "        x2 = self.drop(x2)\n",
    "        x2 = conv_concat(x2, y, self.num_classes)\n",
    "\n",
    "        x3 = self.LReLU(self.conv3(x2))\n",
    "        x3 = conv_concat(x3, y, self.num_classes)\n",
    "\n",
    "        x4 = self.LReLU(self.conv4(x3))\n",
    "        x4 = self.drop(x4)\n",
    "        x4 = conv_concat(x4, y, self.num_classes)\n",
    "\n",
    "        x5 = self.LReLU(self.conv5(x4))\n",
    "        x5 = conv_concat(x5, y, self.num_classes)\n",
    "\n",
    "        x6 = self.LReLU(self.conv6(x5))\n",
    "\n",
    "        x_pool = self.globalPool(x6)\n",
    "\n",
    "        x_pool = x_pool.view(-1, 128 * 4 * 4)\n",
    "        x_out = mlp_concat(x_pool, y, self.num_classes)\n",
    "\n",
    "        out = self.sgmd(self.lin(x_out))\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# discriminator xz\n",
    "class DConvNet2(nn.Module):\n",
    "    '''\n",
    "    2nd convolutional discriminative net (discriminator xz)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_z, channel_in, num_classes, weight_init=True):\n",
    "        super(DConvNet2, self).__init__()\n",
    "\n",
    "        self.logger = logging.getLogger(__name__)  # initialize logger\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # general reusable layers:\n",
    "        self.LReLU = nn.LeakyReLU(negative_slope=0.2)  # leaky ReLU activation function\n",
    "        self.sgmd = nn.Sigmoid()  # sigmoid activation function\n",
    "\n",
    "        # z input -->\n",
    "        self.lin_z0 = nn.Linear(in_features=n_z,\n",
    "                                out_features=512)\n",
    "        # LReLU\n",
    "\n",
    "        self.lin_z1 = nn.Linear(in_features=512,\n",
    "                                out_features=512)\n",
    "        # LReLU\n",
    "\n",
    "        # -------------------------------------\n",
    "\n",
    "        # x input -->\n",
    "        self.conv_x0 = nn.Conv2d(in_channels=channel_in, out_channels=128,\n",
    "                                 kernel_size=(5, 5), stride=2, padding=2, bias=False)\n",
    "        # LReLU\n",
    "\n",
    "        self.conv_x1 = nn.Conv2d(in_channels=128, out_channels=256,\n",
    "                                 kernel_size=(5, 5), stride=2, padding=2, bias=False)\n",
    "        # LReLU\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=256)\n",
    "\n",
    "        self.conv_x2 = nn.Conv2d(in_channels=256, out_channels=512,\n",
    "                                 kernel_size=(5, 5), stride=2, padding=2, bias=False)\n",
    "        # LReLU\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=512)\n",
    "\n",
    "        # -------------------------------------\n",
    "\n",
    "        # concat x & z -->\n",
    "        self.lin_f0 = nn.Linear(in_features=8704,\n",
    "                                out_features=1024)\n",
    "        # LReLU\n",
    "\n",
    "        self.lin_f1 = nn.Linear(in_features=1024,\n",
    "                                out_features=1)\n",
    "        # smg\n",
    "\n",
    "        if weight_init:\n",
    "            # initialize weights for all conv and lin layers\n",
    "            self.apply(init_weights)\n",
    "            # log network structure\n",
    "            self.logger.debug(self)\n",
    "\n",
    "    def forward(self, z, x):\n",
    "        # x: (bs, channel_in, dim_input)\n",
    "        # z: (bs, n_z)\n",
    "\n",
    "        z0 = self.LReLU(self.lin_z0(z))\n",
    "        z_out = self.LReLU(self.lin_z1(z0))\n",
    "\n",
    "        x0 = self.LReLU(self.conv_x0(x))\n",
    "        x1 = self.LReLU(self.conv_x1(x0))\n",
    "        x1 = self.bn1(x1)\n",
    "        x_out = self.LReLU(self.conv_x2(x1))\n",
    "        x_out = self.bn2(x_out)\n",
    "\n",
    "        dims = x_out.size()\n",
    "        fusion = torch.cat([x_out.view(dims[0], -1).squeeze(-1).squeeze(-1), z_out], dim=1)\n",
    "\n",
    "        f_out = self.LReLU(self.lin_f0(fusion))\n",
    "        out = self.sgmd(self.lin_f1(f_out))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classifier module\n",
    "class ClassifierNet(nn.Module):\n",
    "    def __init__(self, in_channels, weight_init=True):\n",
    "        super(ClassifierNet, self).__init__()\n",
    "\n",
    "        self.logger = logging.getLogger(__name__)  # initialize logger\n",
    "\n",
    "        self.gaussian = Gaussian_NoiseLayer()\n",
    "\n",
    "        self.conv1a = nn.Conv2d(in_channels=in_channels, out_channels=128, kernel_size=3,\n",
    "                                stride=1, padding=1)\n",
    "        self.convWN1 = MeanOnlyBatchNorm([1, 128, 32, 32])\n",
    "        self.conv1b = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,\n",
    "                                stride=1, padding=1)\n",
    "        self.conv_relu = nn.LeakyReLU(negative_slope=0.1)\n",
    "        self.conv1c = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,\n",
    "                                stride=1, padding=1)\n",
    "        self.conv_maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.dropout1 = nn.Dropout2d(p=0.5)\n",
    "        self.conv2a = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3,\n",
    "                                stride=1, padding=1)\n",
    "        self.convWN2 = MeanOnlyBatchNorm([1, 256, 16, 16])\n",
    "        self.conv2b = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,\n",
    "                                stride=1, padding=1)\n",
    "        self.conv2c = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,\n",
    "                                stride=1, padding=1)\n",
    "        self.conv_maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.dropout2 = nn.Dropout2d(p=0.5)\n",
    "        self.conv3a = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3,\n",
    "                                stride=1, padding=0)  # output[6,6]\n",
    "        self.convWN3a = MeanOnlyBatchNorm([1, 512, 6, 6])\n",
    "        self.conv3b = nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1,\n",
    "                                stride=1, padding=0)\n",
    "        self.convWN3b = MeanOnlyBatchNorm([1, 256, 6, 6])\n",
    "        self.conv3c = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1,\n",
    "                                stride=1, padding=0)\n",
    "        self.convWN3c = MeanOnlyBatchNorm([1, 128, 6, 6])\n",
    "\n",
    "        self.conv_globalpool = nn.AdaptiveAvgPool2d(6)\n",
    "\n",
    "        self.dense = nn.Linear(in_features=128 * 6 * 6, out_features=10)\n",
    "        self.smx = nn.Softmax()\n",
    "        #self.WNfinal = MeanOnlyBatchNorm([1, 128, 6, 6])\n",
    "\n",
    "        if weight_init:\n",
    "            # initialize weights for all conv and lin layers\n",
    "            self.apply(init_weights)\n",
    "            # log network structure\n",
    "            self.logger.debug(self)\n",
    "\n",
    "    def forward(self, x, cuda):\n",
    "        x = self.gaussian(x, cuda=cuda)\n",
    "        x = self.convWN1(self.conv_relu(self.conv1a(x)))\n",
    "        x = self.convWN1(self.conv_relu(self.conv1b(x)))\n",
    "        x = self.convWN1(self.conv_relu(self.conv1c(x)))\n",
    "        x = self.conv_maxpool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.convWN2(self.conv_relu(self.conv2a(x)))\n",
    "        x = self.convWN2(self.conv_relu(self.conv2b(x)))\n",
    "        x = self.convWN2(self.conv_relu(self.conv2c(x)))\n",
    "        x = self.conv_maxpool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.convWN3a(self.conv_relu(self.conv3a(x)))\n",
    "        x = self.convWN3b(self.conv_relu(self.conv3b(x)))\n",
    "        x = self.convWN3c(self.conv_relu(self.conv3c(x)))\n",
    "        x = self.conv_globalpool(x)\n",
    "        x = x.view(-1, 128 * 6 * 6)\n",
    "        #x = self.WNfinal(self.smx(self.dense(x)))\n",
    "        x = self.smx(self.dense(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class InferenceNet(nn.Module):\n",
    "    def __init__(self, in_channels, n_z, weight_init=True):\n",
    "        super(InferenceNet, self).__init__()\n",
    "\n",
    "        self.logger = logging.getLogger(__name__)  # initialize logger\n",
    "\n",
    "        self.inf02 = nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=4,\n",
    "                               stride=2, padding=1)\n",
    "        self.inf03 = nn.BatchNorm2d(64)\n",
    "        self.inf11 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4,\n",
    "                               stride=2, padding=1)\n",
    "        self.inf12 = nn.BatchNorm2d(128)\n",
    "        self.inf21 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4,\n",
    "                               stride=2, padding=1)\n",
    "        self.inf22 = nn.BatchNorm2d(256)\n",
    "        self.inf31 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4,\n",
    "                               stride=2, padding=1)\n",
    "        self.inf32 = nn.BatchNorm2d(512)\n",
    "        self.inf4 = nn.Linear(in_features=512*2*2, out_features=n_z)\n",
    "\n",
    "        if weight_init:\n",
    "            # initialize weights for all conv and lin layers\n",
    "            self.apply(init_weights)\n",
    "            # log network structure\n",
    "            self.logger.debug(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.inf03(self.inf02(x)))\n",
    "        x = F.leaky_relu(self.inf12(self.inf11(x)))\n",
    "        x = F.leaky_relu(self.inf22(self.inf21(x)))\n",
    "        x = F.leaky_relu(self.inf32(self.inf31(x)))\n",
    "        x = x.view(-1, 512*2*2)\n",
    "        x = self.inf4(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### INITS ###\n",
    "\n",
    "# GENRATOR\n",
    "generator = Generator(input_size=110, num_classes=NUM_CLASSES, dense_neurons=(4 * 4 * 512))\n",
    "\n",
    "# INFERENCE\n",
    "inference = InferenceNet(in_channels=IN_CHANNELS, n_z=N_Z)\n",
    "\n",
    "# CLASSIFIER\n",
    "classifier = ClassifierNet(in_channels=IN_CHANNELS)\n",
    "\n",
    "# DISCRIMINATOR\n",
    "discriminator1 = DConvNet1(channel_in=IN_CHANNELS, num_classes=NUM_CLASSES)\n",
    "discriminator2 = DConvNet2(n_z=N_Z, channel_in=IN_CHANNELS, num_classes=NUM_CLASSES)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put on GPU\n",
    "if CUDA:\n",
    "    generator.cuda()\n",
    "    inference.cuda()\n",
    "    classifier.cuda()\n",
    "    discriminator1.cuda()\n",
    "    discriminator2.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ZCA\n",
    "whitener = ZCA(x=x_unlabelled)\n",
    "\n",
    "# LOSS FUNCTIONS\n",
    "if CUDA:\n",
    "    losses = {\n",
    "        'bce': nn.BCELoss().cuda(),\n",
    "        'mse': nn.MSELoss().cuda(),\n",
    "        'ce': nn.CrossEntropyLoss().cuda()\n",
    "    }\n",
    "else:\n",
    "    losses = {\n",
    "        'bce': nn.BCELoss(),\n",
    "        'mse': nn.MSELoss(),\n",
    "        'ce': nn.CrossEntropyLoss()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### PRETRAIN CLASSIFIER ###\n",
    "\n",
    "logger.info('Start pretraining...')\n",
    "for epoch in range(1, 1+NUM_EPOCHS_PRE):\n",
    "\n",
    "    # pretrain classifier net\n",
    "    classifier = pretrain_classifier(x_labelled, x_unlabelled, y_labelled, eval_x, eval_y, num_batches_l,\n",
    "                                     BATCH_SIZE, num_batches_u, classifier, whitener, losses, rng, CUDA)\n",
    "\n",
    "    # evaluate\n",
    "    accurracy = eval_classifier(num_batches_e, eval_x, eval_y, BATCH_SIZE_EVAL, whitener, classifier, CUDA)\n",
    "\n",
    "    logger.info(str(epoch) + ':Pretrain error_rate: ' + str(1 - accurracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "### GAN TRAINING ###\n",
    "\n",
    "# assign start values\n",
    "lr_cla = LR_CLA\n",
    "lr = LR\n",
    "start_full = time.time()\n",
    "\n",
    "logger.info(\"Start GAN training...\")\n",
    "for epoch in range(1, 1+NUM_EPOCHS):\n",
    "\n",
    "    # OPTIMIZERS\n",
    "    optimizers = {\n",
    "        'dis': optim.Adam(list(discriminator1.parameters()) + list(discriminator2.parameters()), betas=(B1, 0.999), lr=lr),\n",
    "        'gen': optim.Adam(generator.parameters(), betas=(B1, 0.999), lr=lr),\n",
    "        'inf': optim.Adam(inference.parameters(), betas=(B1, 0.999), lr=lr)\n",
    "    }\n",
    "\n",
    "    # randomly permute data and labels each epoch\n",
    "    p_l = rng.permutation(x_labelled.shape[0])\n",
    "    x_labelled = x_labelled[p_l]\n",
    "    y_labelled = y_labelled[p_l]\n",
    "\n",
    "    # permuted slicer objects\n",
    "    p_u = rng.permutation(x_unlabelled.shape[0]).astype('int32')\n",
    "    p_u_d = rng.permutation(x_unlabelled.shape[0]).astype('int32')\n",
    "    p_u_i = rng.permutation(x_unlabelled.shape[0]).astype('int32')\n",
    "\n",
    "    # set epoch dependent values\n",
    "    if epoch < (NUM_EPOCHS/2):\n",
    "        if epoch % 50 == 1:\n",
    "            batch_l = 200 - (epoch // 50 + 1) * 16\n",
    "            batch_c = (epoch // 50 + 1) * 16\n",
    "            batch_g = 1\n",
    "    elif epoch < NUM_EPOCHS and epoch % 100 == 0:\n",
    "        batch_l = 50\n",
    "        batch_c = 140 - 10 * (epoch-500)/100\n",
    "        batch_g = 10 + 10 * (epoch-500)/100\n",
    "\n",
    "    # if current epoch is an evaluation epoch, train classifier and report results\n",
    "    if epoch % EVAL_EPOCH == 0:\n",
    "\n",
    "        logger.info('Train classifier...')\n",
    "\n",
    "        rampup_value = rampup(epoch-1)\n",
    "        rampdown_value = rampdown(epoch-1)\n",
    "        b1_c = rampdown_value * 0.9 + (1.0 - rampdown_value) * 0.5\n",
    "        unsup_weight = rampup_value * SCALED_UNSUP_WEIGHT_MAX if epoch > 1 else 0.0\n",
    "        w_g = np.float32(min(float(epoch) / 300.0, 1.0))\n",
    "\n",
    "        size_l = 100\n",
    "        size_g = 100\n",
    "        size_u = 100\n",
    "\n",
    "        cla_losses = train_classifier(x_labelled=x_labelled,\n",
    "                                      y_labelled=y_labelled,\n",
    "                                      x_unlabelled=x_unlabelled,\n",
    "                                      num_batches_u=num_batches_u,\n",
    "                                      eval_epoch=EVAL_EPOCH,\n",
    "                                      size_l=size_l,\n",
    "                                      size_u=size_u,\n",
    "                                      size_g=size_g,\n",
    "                                      n_z=N_Z,\n",
    "                                      whitener=whitener,\n",
    "                                      classifier=classifier,\n",
    "                                      p_u=p_u,\n",
    "                                      unsup_weight=unsup_weight,\n",
    "                                      losses=losses,\n",
    "                                      generator=generator,\n",
    "                                      w_g=w_g,\n",
    "                                      cla_lr=lr_cla,\n",
    "                                      rng=rng,\n",
    "                                      b1_c=b1_c,\n",
    "                                      cuda=CUDA)\n",
    "\n",
    "        # evaluate & report\n",
    "        accurracy = eval_classifier(num_batches_e, eval_x, eval_y, BATCH_SIZE_EVAL, whitener, classifier, CUDA)\n",
    "\n",
    "        logger.info('Evaluation error_rate: %.5f\\n' % (1 - accurracy))\n",
    "\n",
    "    logger.info('Train generator, inference and discriminator model...')\n",
    "    # train GAN model\n",
    "    for i in range(num_batches_u):\n",
    "        gan_losses = train_gan(discriminator1=discriminator1,\n",
    "                               discriminator2=discriminator2,\n",
    "                               generator=generator,\n",
    "                               inferentor=inference,\n",
    "                               classifier=classifier,\n",
    "                               whitener=whitener,\n",
    "                               x_labelled=x_labelled,\n",
    "                               x_unlabelled=x_unlabelled,\n",
    "                               y_labelled=y_labelled,\n",
    "                               p_u_d=p_u_d,\n",
    "                               p_u_i=p_u_i,\n",
    "                               num_classes=NUM_CLASSES,\n",
    "                               batch_size=BATCH_SIZE,\n",
    "                               num_batches_u=num_batches_u,\n",
    "                               batch_c=batch_c,\n",
    "                               batch_l=batch_l,\n",
    "                               batch_g=batch_g,\n",
    "                               n_z=N_Z,\n",
    "                               optimizers=optimizers,\n",
    "                               losses=losses,\n",
    "                               rng=rng,\n",
    "                               cuda=CUDA)\n",
    "\n",
    "    # anneal the learning rates\n",
    "    if (epoch >= ANNEAL_EPOCH) and (epoch % ANNEAL_EVERY_EPOCH == 0):\n",
    "        lr = lr * ANNEAL_FACTOR\n",
    "        lr_cla *= ANNEAL_FACTOR_CLA\n",
    "\n",
    "    # report and log training info\n",
    "    t = time.time() - start_full\n",
    "    line = \"*Epoch=%d Time=%.2f LR=%.5f\\n\" % (epoch, t, lr) + \"DisLosses: \" + str(gan_losses['dis']) + \"\\nGenLosses: \" + \\\n",
    "           str(gan_losses['gen']) + \"\\nInfLosses: \" + str(gan_losses['inf']) + \"\\nClaLosses: \" + str(cla_losses)\n",
    "    logger.info(line)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
